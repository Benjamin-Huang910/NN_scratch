---
title: "Multi-Layer NN Notes"
author: "Daniel Polites"
output: pdf_document
---

```{=tex}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\err}{\hat \epsilon_i}
\newcommand{\errvec}{\mathbf{\hat \epsilon}}
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
set.seed(1000)
```

# Notation Setup

source: <https://arxiv.org/abs/1801.05894>

---

## Scalars

Layers: 1-$L$, indexed by $l$

Number of Neurons in layer $l$: $n_l$

Neuron Activations: $a^{(\text{layer num})}_{\text{neuron num}} = a^{(l)}_j$. Vector of activations for a layer is $a^{(l)}$

Activation Function: $g(\cdot)$ is our generic activation function

---

## X

We have our input matrix $X \in \R^{\text{vars} \times \text{obs}} = \R^{n_0 \times m}$:

$$
X = \ ^{n_0 \text{ outputs}}
\overbrace{
  \begin{cases}
    \begin{bmatrix}
    x_{1, 1} & x_{1, 2} & \cdots & x_{1, m} \\
    x_{2, 1} & x_{2, 2} & \cdots & x_{2, m} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{n_0, 1} & x_{n_0, 2} & \cdots & x_{n_0, m} \\
    \end{bmatrix}
  \end{cases} 
}^{m \text{ obs}}
$$

------------------------------------------------------------------------

## W

our Weight matrices $W^{(l)} \in \R^{\text{out} \times \text{in}} = \R^{n_l \times n_{l - 1}}$:

$$
W^{(l)} = \ ^{n_l\text{ outputs}}
\overbrace{
  \begin{cases}
    \begin{bmatrix}
    w^{(l)}_{1, 1} & w^{(l)}_{1, 2} & \cdots & w^{(l)}_{1, n_{l-1}} \\
    w^{(l)}_{2, 1} & w^{(l)}_{2, 2} & \cdots & w^{(l)}_{2, n_{l-1}} \\
    \vdots & \vdots & \ddots & \vdots \\
    w^{(l)}_{n_l, 1} & w^{(l)}_{n_l, 2} & \cdots & w^{(l)}_{n_l, n_{l-1}}
    \end{bmatrix}
  \end{cases} 
}^{n_{l - 1} \text{ inputs}}
$$

$W^{(l)}$ is the weight matrix for the $l$th layer

------------------------------------------------------------------------

## b

our Bias matrices $b^{(l)} \in \R^{\text{out} \times 1} = \R^{n_l \times 1}$:

$$
b^{(l)} = \ ^{n_l\text{ outputs}}
  \begin{cases}
    \begin{bmatrix}
    b^{(l)}_{1} \\
    b^{(l)}_{2} \\
    \vdots \\
    b^{(l)}_{n_l}
    \end{bmatrix}
  \end{cases}
$$

$b^{(l)}$ is the bias matrix for the $l$th layer

------------------------------------------------------------------------

## Y

our target layer matrix $Y \in \R^{\text{cats} \times \text{obs}} = \R^{n_L \times m}$:

$$
Y = \ ^{n_L \text{ categories}}
\overbrace{
  \begin{cases}
    \begin{bmatrix}
    y_{1, 1} & y_{1, 2} & \cdots & y_{1, m} \\
    y_{2, 1} & y_{2, 2} & \cdots & y_{2, m} \\
    \vdots & \vdots & \ddots & \vdots \\
    y_{n_L, 1} & y_{n_L, 2} & \cdots & y_{n_L, m}
    \end{bmatrix}
  \end{cases} 
}^{m \text{ obs}}
$$

------------------------------------------------------------------------

## z

our neuron layer's activation function input $z^{(l)} \in \R^{\text{out} \times 1} = \R^{n_l \times 1}$:

$$
z^{(l)} = \ ^{n_l\text{ outputs}}
  \begin{cases}
    \begin{bmatrix}
      z^{(l)}_{1} \\
      z^{(l)}_{2} \\
      \vdots \\
      z^{(l)}_{n_l}
    \end{bmatrix}
  \end{cases}
$$

$z^{(l)}$ is the neuron 'weighted input' matrix for the $l$th layer

We have that:

$$
\begin{aligned}
z^{(l)} &= W^{(l)} \cdot a^{(l - 1)} + b^{(l)} \\ \\
&= \ ^{n_l\text{ outputs}}
\overbrace{
  \begin{cases}
    \begin{bmatrix}
    w^{(l)}_{1, 1} & w^{(l)}_{1, 2} & \cdots & w^{(l)}_{1, n_{l-1}} \\
    w^{(l)}_{2, 1} & w^{(l)}_{2, 2} & \cdots & w^{(l)}_{2, n_{l-1}} \\
    \vdots & \vdots & \ddots & \vdots \\
    w^{(l)}_{n_l, 1} & w^{(l)}_{n_l, 2} & \cdots & w^{(l)}_{n_l, n_{l-1}}
    \end{bmatrix}
  \end{cases} 
}^{n_{l - 1} \text{ inputs}} \cdot \ ^{n_{l - 1} \text{ inputs}}
  \begin{cases}
    \begin{bmatrix}
      a^{(l-1)}_{1} \\
      a^{(l-1)}_{2} \\
      \vdots \\
      a^{(l-1)}_{n_l}
    \end{bmatrix}
  \end{cases} + \ ^{n_l\text{ outputs}}
  \begin{cases}
    \begin{bmatrix}
    b^{(l)}_{1} \\
    b^{(l)}_{2} \\
    \vdots \\
    b^{(l)}_{n_l}
    \end{bmatrix}
  \end{cases} \\ \\
&= \ ^{n_l\text{ outputs}}
  \begin{cases}
    \begin{bmatrix}
      z^{(l)}_{1} \\
      z^{(l)}_{2} \\
      \vdots \\
      z^{(l)}_{n_l}
    \end{bmatrix}
  \end{cases}
\end{aligned}
$$

------------------------------------------------------------------------

## a

our Neuron Activation $a^{(l)} \in \R^{\text{out} \times 1} = \R^{n_l \times 1}$:

$$
a^{(l)} = \ ^{n_l\text{ outputs}}
  \begin{cases}
    \begin{bmatrix}
      a^{(l)}_{1} \\
      a^{(l)}_{2} \\
      \vdots \\
      a^{(l)}_{n_l}
    \end{bmatrix}
  \end{cases}
$$

$a^{(l)}$ is the activation matrix for the $l$th layer

We have that:

$$
\begin{aligned}
a^{(l)} &= g\left(z^{(l)}\right) \\ \\
&= g\left(W^{(l)} \cdot a^{(l - 1)} + b^{(l)}\right) \\ \\
&= g\left(\ ^{n_l\text{ outputs}}
\overbrace{
  \begin{cases}
    \begin{bmatrix}
    w^{(l)}_{1, 1} & w^{(l)}_{1, 2} & \cdots & w^{(l)}_{1, n_{l-1}} \\
    w^{(l)}_{2, 1} & w^{(l)}_{2, 2} & \cdots & w^{(l)}_{2, n_{l-1}} \\
    \vdots & \vdots & \ddots & \vdots \\
    w^{(l)}_{n_l, 1} & w^{(l)}_{n_l, 2} & \cdots & w^{(l)}_{n_l, n_{l-1}}
    \end{bmatrix}
  \end{cases} 
}^{n_{l - 1} \text{ inputs}} \cdot \ ^{n_{l - 1} \text{ inputs}}
  \begin{cases}
    \begin{bmatrix}
      a^{(l-1)}_{1} \\
      a^{(l-1)}_{2} \\
      \vdots \\
      a^{(l-1)}_{n_l}
    \end{bmatrix}
  \end{cases} + \ ^{n_l\text{ outputs}}
  \begin{cases}
    \begin{bmatrix}
    b^{(l)}_{1} \\
    b^{(l)}_{2} \\
    \vdots \\
    b^{(l)}_{n_l}
    \end{bmatrix}
  \end{cases}\right) \\ \\
&= g\left(\ ^{n_l\text{ outputs}}
  \begin{cases}
    \begin{bmatrix}
      z^{(l)}_{1} \\
      z^{(l)}_{2} \\
      \vdots \\
      z^{(l)}_{n_l}
    \end{bmatrix}
  \end{cases}\right) \\ \\
&= \ ^{n_l\text{ outputs}}
  \begin{cases}
    \begin{bmatrix}
      a^{(l)}_{1} \\
      a^{(l)}_{2} \\
      \vdots \\
      a^{(l)}_{n_l}
    \end{bmatrix}
  \end{cases}
\end{aligned}
$$

# Forward Propagation

a
