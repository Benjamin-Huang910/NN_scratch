---
title: "ML_NN"
Author: "Benjamin Huang"
date: "2024-03-29"
---

```{r Data}
library(tidyverse)
## create data:
m <- 1000
n_1_manual <- 3
n_L_manual <- 1

# initialize Xs
X <- data.frame(X1 = runif(n = m, min = -10, max = 10),
                X2 = rnorm(n = m, mean = 0, sd = 10),
                X3 = rexp(n = m, rate = 1)) %>%
  as.matrix(nrow = m,
            ncol = n_1_manual)

# get response
Y <- X[, 1] + 10 * sin(X[, 2])^2 + 10 * X[, 3] + rnorm(n = 1000)

# fix dims according to NN specs
X <- t(X)
Y <- t(Y)
```

```{r Link Function}
## Specify Link Functions & Derivatives
get_link <- function(type = "sigmoid") {
  
  if (type == "identity") {
    # identity
    g <- function(x) {x}
    
  } else if (type == "sigmoid") {
    # sigmoid
    g <- function(x) {1 / (1 + exp(-x))}
    
  } else if (type == "softmax") {
    # softmax
    g <- function(x) {
      exp_x <- exp(x - max(x))  # Subtracting max(x) for numerical stability
      return(exp_x / sum(exp_x))
    }
    
  } else if (type == "relu") {
    # ReLU
    g <- function(x) {x * as.numeric(x > 0)}
    
  } else (return(NULL))
  
  return(g)

}

get_link_prime <- function(type = "sigmoid") {
  
  if (type == "identity") {
    # identity [FIX]
    g_prime <- function(x) {rep(1, length(x))}
    
  } else if (type == "sigmoid") {
    # sigmoid
    g_prime <- function(x) {exp(-x) / (1 + exp(-x))^2}
    
  } else if (type == "softmax") {
    # Derivative of softmax
    g_prime <- function(x) {
      s <- get_link("softmax")(x)
      return(s * (1 - s))
    }
  } else if (type == "relu") {
    # ReLU
    g_prime <- function(x) {as.numeric(x > 0)}
    
  } else (return(NULL))
  
  return(g_prime)

}
```

```{r Loss Function}
## Specify Loss Functions & Derivatives
get_loss_function <- function(type = "squared_error") {
  
  if (type == "squared_error") {
    
    loss <- function(y_hat, y) {sum((y_hat - y)^2)}
    
  } else if (type == "absolute_error") {
    
    loss <- function(y_hat, y) {sum(abs(y_hat - y))}
    
  } else if (type == "binary_cross_entropy") {
    
    loss <- function(y_hat, y) {-(y * log(y_hat) + (1-y) * log(1 - y_hat))}
    
  } else if (type == "categorical_cross_entropy") {
    
    loss <- function(y_hat, y) {-sum(y * log(y_hat))}
    
  } else (return(NULL))
  
  return(loss)

}

get_loss_prime <- function(type = "squared_error") {
  
  if (type == "squared_error") {
    
    loss_prime <- function(y_hat, y) {sum(2 * (y_hat - y))}
    
  } else if (type == "absolute_error") {
    
    loss_prime <- function(y_hat, y) {sum(sign(y_hat - y))}
    
  } else if (type == "binary_cross_entropy") {
    
    loss_prime <- function(y_hat, y) {-((y / y_hat) - ((1 - y) / (1 - y_hat)))}
    
  } else if (type == "categorical_cross_entropy") {
    
    loss_prime <- function(y_hat, y) {-sum(y / y_hat)}
    
  } else (return(NULL))
  
  return(loss_prime)

}
```

```{r Misc Helpers}
## creates a list of n empty lists
create_lists <- function(n) {
  out <- list()
  
  for (i in 1:n) {
    out[[i]] <- list()
  }
  
  return(out)
}

## friendlier diag() function
diag_D <- function(x) {
  
  if (length(x) == 1) {
        out <- x
      } else {
        out <- diag(as.numeric(x))
      }
  
  return(out)
}

fetch_layer_sizes <- function(X, Y, hidden_layer_sizes) {
  
  return(c(nrow(X), hidden_layers, nrow(Y)))
  
}
```

```{r Initialization}
initialize_NN <- function(layer_sizes,
                          activation_function = "identity",
                          last_activation_function = "softmax",
                          lower_bound = 0,
                          upper_bound = 1) {
  
  n <- layer_sizes
  
  ## initialize parameter matrices
  W <- list()
  b <- list()
  
  ## Initialize parameter matrices using mapply()
  W <- mapply(function(rows, cols) 
              matrix(runif(rows * cols, lower_bound, upper_bound), 
              nrow = rows, 
              ncol = cols),
              n[-length(n)], n[-1])
  
  b <- mapply(function(rows) 
              matrix(runif(rows, lower_bound, upper_bound), 
              nrow = rows, 
              ncol = 1),
              n[-1])
  
  ## return
  return(list(W = W,
              b = b,
              activation_function = activation_function,
              last_activation_function = last_activation_function))
}
```

```{r Forward Propagation}
NN_output <- function(X,
                      NN_obj) {
  
  L <- length(NN_obj$W)
  ## if X is one obs, input will be a vector so dim will be null
  m <- ifelse(is.null(ncol(X)),
              1,
              ncol(X))
  
  g <- get_link(NN_obj$activation_function)
  g_last <- get_link(NN_obj$last_activation_function)
  
  a <- list()
  
  a[[1]] <- X
  
  for (l in 2:(L - 1)) {
    a[[l]] <- g(NN_obj$W[[l]] %*% a[[l - 1]] + matrix(data = rep(x = NN_obj$b[[l]],
                                                                 times = m),
                                                      ncol = m))
  }
  
  a[[L]] <- g_last(NN_obj$W[[L]] %*% a[[L - 1]] + matrix(data = rep(x = NN_obj$b[[L]],
                                                                    times = m),
                                                         ncol = m))
  
  return(a[[L]])
  
}
```

```{r}
GD_iter <- function(NN_obj,
                    X,
                    Y,
                    rho = 1,
                    verbose = FALSE,
                    very_verbose = FALSE) {
  
  L <- length(NN_obj$W)
  ## if X is one obs, input will be a vector so dim will be null
  m <- ifelse(is.null(ncol(X)),
              1,
              ncol(X))
  
  ## get links
  g <- get_link(NN_obj$activation_function)
  g_prime <- get_link_prime(NN_obj$activation_function)
  g_last <- get_link(NN_obj$last_activation_function)
  g_last_prime <- get_link_prime(NN_obj$last_activation_function)
  
  z <- create_lists(L)
  a <- create_lists(L)
  D <- create_lists(L)
  delta <- create_lists(L)
  del_W <- create_lists(L)
  del_b <- create_lists(L)
  
  ## gradient descent
  for (i in 1:m) {
    
    ## forward
    a[[1]][[i]] <- X[, i]
    
    for (l in 2:(L - 1)) {
      z[[l]][[i]] <- NN_obj$W[[l]] %*% a[[l - 1]][[i]] + NN_obj$b[[l]]
      a[[l]][[i]] <- g(z[[l]][[i]])
      D[[l]][[i]] <- diag_D(g_prime(z[[l]][[i]]))
      
      if (very_verbose == TRUE) {print(paste0("Forward: obs ", i, " - layer ", l))}
    }
    
    ## last layer
    z[[L]][[i]] <- NN_obj$W[[L]] %*% a[[L - 1]][[i]] + NN_obj$b[[L]]
    a[[L]][[i]] <- g_last(z[[L]][[i]])
    D[[L]][[i]] <- diag_D(g_last_prime(z[[L]][[i]]))
    
    ## backward
    # eventually fix to match with loss function
    delta[[L]][[i]] <- D[[L]][[i]] %*% (a[[L]][[i]] - Y[, i])
    
    for (l in (L - 1):2) {
      delta[[l]][[i]] <- D[[l]][[i]] %*% t(NN_obj$W[[l + 1]]) %*% delta[[l + 1]][[i]]
      if (very_verbose == TRUE) {print(paste0("Backward: obs ", i, " - layer ", l))}
    }
    
    for (l in 2:L) {
      del_W[[l]][[i]] <- delta[[l]][[i]] %*% t(a[[l - 1]][[i]])
      del_b[[l]][[i]] <- delta[[l]][[i]]
      if (very_verbose == TRUE) {print(paste0("del: obs ", i, " - layer ", l))}
    }
    
    if ((verbose == TRUE) & (i %% 100 == 0)) {print(paste("obs", i, "/", m))}
    
  }
  
  ## update parameters
  
  # get averages
  ## del_W is a list where each element represents a layer
  ## in each layer, there's a list representing the layer's result for that obs
  ## here we collapse the results by taking the sum of our gradients
  del_W_all <- lapply(X = del_W,
                      FUN = Reduce,
                      f = "+") %>%
    lapply(X = .,
           FUN = function(x) x / m)
  
  del_b_all <- lapply(X = del_b,
                      FUN = Reduce,
                      f = "+") %>%
    lapply(X = .,
           FUN = function(x) x / m)
  
  # apply gradient
  W_out <- mapply(FUN = function(A, del_A) {A - rho * del_A},
                  A = NN_obj$W,
                  del_A = del_W_all)
  
  b_out <- mapply(FUN = function(A, del_A) {A - rho * del_A},
                  A = NN_obj$b,
                  del_A = del_b_all)
  
  ## return a new NN object
  return(list(W = W_out,
              b = b_out,
              activation_function = NN_obj$activation_function,
              last_activation_function = NN_obj$last_activation_function))
}
```